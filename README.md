Aqui est√° o arquivo `README.md` completo, profissional e formatado para o seu portf√≥lio. Ele documenta toda a arquitetura que constru√≠mos, desde a ingest√£o at√© a API.

Voc√™ pode copiar o c√≥digo abaixo e salvar como `README.md` na raiz do projeto.


# üõí Real-Time Analytics Hub

> **Arquitetura de Refer√™ncia para Engenharia de Dados em Tempo Real** > Processamento de transa√ß√µes de e-commerce com Apache Spark Streaming, Kafka, Delta Lake e Django.

![Status](https://img.shields.io/badge/Status-MVP%20Fase%201%20Conclu√≠da-success?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-7.6-231F20?style=for-the-badge&logo=apachekafka&logoColor=white)
![Django](https://img.shields.io/badge/Django-5.0-092E20?style=for-the-badge&logo=django&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)

---

## üèóÔ∏è Arquitetura do Sistema

O projeto simula um ecossistema de dados de alta performance, capaz de ingerir, processar e servir dados de vendas em tempo real.

```mermaid
graph LR
    subgraph Ingest√£o
    A[Mock Generator] -->|JSON Events| B(Kafka Broker)
    end
    
    subgraph Processamento
    B -->|Stream| C{Spark Structured Streaming}
    C -->|Agrega√ß√£o 1 min| D[(MinIO / Delta Lake)]
    end
    
    subgraph Serving
    D -->|Leitura Parquet| E[Django API]
    E -->|WebSockets| F[Dashboard React]
    end
```

## üöÄ Funcionalidades

### 1. Ingest√£o de Dados (Mock Generator)

* Simula transa√ß√µes de m√∫ltiplas plataformas (Amazon, Mercado Livre, Shopee).
* Gera eventos estoc√°sticos de **Vendas**, **Atualiza√ß√µes de Estoque** e **Visualiza√ß√µes**.
* Publica mensagens em formato JSON no t√≥pico `ecommerce-events` do Kafka.

### 2. Processamento em Stream (Spark Jobs)

* **Engine:** Apache Spark 3.5 com Delta Lake 3.0.
* **Job de Agrega√ß√£o:** Calcula a receita total por plataforma em janelas deslizantes de 1 minuto.
* **Job de Alertas:** Detecta estoque cr√≠tico (< 10 unidades) e vendas de alto valor (High Ticket) em tempo real.
* **Checkpoints:** Garante toler√¢ncia a falhas e processamento *exactly-once*.

### 3. Data Lakehouse (MinIO + Delta Lake)

* Armazenamento de dados brutos e processados em formato **Parquet/Delta**.
* Suporte a *Time Travel* e transa√ß√µes ACID.
* Estrutura de pastas particionada para otimiza√ß√£o de leitura.

### 4. Backend & API (Django)

* **Leitura de Data Lake:** Servi√ßo customizado para ler arquivos Parquet diretamente do MinIO.
* **API REST:** Endpoints para consulta de m√©tricas consolidadas.
* **WebSockets:** Implementa√ß√£o com Django Channels para push de atualiza√ß√µes ao frontend.

---

## üõ†Ô∏è Stack Tecnol√≥gica

| Componente | Tecnologia | Vers√£o | Descri√ß√£o |
| --- | --- | --- | --- |
| **Linguagem** | Python | 3.12 | Base de todos os servi√ßos (uv workspace). |
| **Broker** | Apache Kafka | 7.6 | Backbone de mensageria da Confluent. |
| **Engine** | PySpark | 3.5.0 | Processamento distribu√≠do e Streaming. |
| **Storage** | MinIO | Latest | Object Storage compat√≠vel com S3. |
| **Formato** | Delta Lake | 3.0.0 | Camada de armazenamento open table. |
| **Backend** | Django | 5.0 | Framework Web e API. |
| **Infra** | Docker | Compose | Orquestra√ß√£o de containers local. |

---

## ‚öôÔ∏è Como Rodar o Projeto

### Pr√©-requisitos

* Docker e Docker Compose instalados.
* Gerenciador de pacotes `uv` (opcional, para desenvolvimento local).

### Passo 1: Subir a Infraestrutura

Na raiz do projeto, execute:

```bash
docker-compose up -d --build

```

*Isso iniciar√° o Kafka, Zookeeper, MinIO, Postgres, Redis, Spark Master/Worker e a API Django.*

### Passo 2: Iniciar o Job do Spark

O cluster Spark sobe em modo "Standby". Para iniciar o processamento do stream:

1. Acesse o container do Worker:
```bash
docker exec -it ecommerce_spark_worker bash

```


2. Submeta o job de agrega√ß√£o:
```bash
/opt/bitnami/spark/bin/spark-submit \
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4 \
--master spark://spark-master:7077 \
/opt/spark-jobs/streaming_sales_agg.py

```



### Passo 3: Acessar os Servi√ßos

| Servi√ßo | URL | Credenciais (Padr√£o) |
| --- | --- | --- |
| **MinIO Console** | http://localhost:9001 | `minioadmin` / `minioadmin` |
| **Spark Master UI** | http://localhost:8080 | - |
| **API Health Check** | http://localhost:8000/api/health/ | - |
| **Django Admin** | http://localhost:8000/admin/ | (Requer `createsuperuser`) |

---

## üìÇ Estrutura do Reposit√≥rio (Monorepo)

```text
realtime-analytics-hub/
‚îú‚îÄ‚îÄ backend/                # API Django e Consumers
‚îÇ   ‚îú‚îÄ‚îÄ api/                # Apps e L√≥gica de Neg√≥cio
‚îÇ   ‚îú‚îÄ‚îÄ config/             # Configura√ß√µes do Projeto (settings, asgi)
‚îÇ   ‚îî‚îÄ‚îÄ services/           # Leitores do Delta Lake
‚îú‚îÄ‚îÄ mock-generator/         # Gerador de dados fake (Producer)
‚îÇ   ‚îú‚îÄ‚îÄ platforms/          # L√≥gica espec√≠fica de cada e-commerce
‚îÇ   ‚îî‚îÄ‚îÄ generator.py        # Script principal
‚îú‚îÄ‚îÄ spark-jobs/             # Scripts PySpark (Processor)
‚îÇ   ‚îú‚îÄ‚îÄ streaming_sales_agg.py
‚îÇ   ‚îî‚îÄ‚îÄ alert_detection.py
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestra√ß√£o
‚îú‚îÄ‚îÄ pyproject.toml          # Defini√ß√£o do Workspace UV
‚îî‚îÄ‚îÄ README.md               # Documenta√ß√£o

```

## üêõ Troubleshooting Comum

**O Spark n√£o salva dados no MinIO?**

* Verifique se o bucket `delta-lake` foi criado no MinIO (http://localhost:9001).
* Confirme se as credenciais no `.env` s√£o `minioadmin`.

**Erro de Offset no Kafka?**

* Se reiniciar o cluster, o Spark pode tentar ler offsets antigos. Altere a op√ß√£o `.option("startingOffsets", "earliest")` no script Python.

---

Desenvolvido por **Paulo Arruda**.

```